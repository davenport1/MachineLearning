{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2acb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class: COEN 140 Lab Machine Learning/Data Mining\n",
    "# Name: Matthew Davenport\n",
    "# Date: 9/25/2022\n",
    "# Title: Lab 8 â€“ K-Means Clustering\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# as directed by lab doc:\n",
    "num_of_clusters = 10\n",
    "num_of_initializations = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34e1468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File unpickled successfully\n",
      "(2000, 784)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "try: \n",
    "    with open(\"data.pickle\", \"rb\") as fp:\n",
    "        X_training = pickle.load(fp)\n",
    "    print(\"File unpickled successfully\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X_training = mnist[\"data\"][:2000]\n",
    "    \n",
    "    with open(\"data.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(X_training, fp)\n",
    "\n",
    "print(X_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb18c5",
   "metadata": {},
   "source": [
    "my_kmeans performs a k-means clustering of the 2000 images of digits.\n",
    "Takes 3 arguments\n",
    "\n",
    "data: the data matrix\n",
    "\n",
    "K: the number of clusters\n",
    "\n",
    "M: the number of initializations\n",
    "\n",
    "Returns: \n",
    "\n",
    "(1) the K centroids and cluster assignments for the best solution with the lowest loss \n",
    "function (recall that the k-means loss function is the sum of the squared distances of \n",
    "observations from their assigned means) \n",
    "\n",
    "(2) the sequence of values of the loss-function over k-means iterations for the best solution \n",
    "(this should be non-increasing) \n",
    "\n",
    "(3) the set of M terminal loss-function values for all initializations   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8893c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# def my_kmeans(data, K, M):\n",
    "#     # return lists: \n",
    "#     return_clusters = []      # holds clusters to be returned \n",
    "#     return_centroids = []     # centroids to be returned \n",
    "#     return_losses = [] \n",
    "#     return_final_losses = []  # final losses realized\n",
    "    \n",
    "#     data_size = len(data)     # number of points in our dataset\n",
    "    \n",
    "#     mean_square_errors = []   # list of mean square errors realized\n",
    "    \n",
    "#     data = np.asarray(data)    # convert data to np array\n",
    "#     # M initializations\n",
    "#     for i in range(M): \n",
    "        \n",
    "#         initializations = np.random.choice(2000, K, replace=False) # different inits per M iterations\n",
    "        \n",
    "#         closest_clusters = np.zeros(data_size)\n",
    "#         temp_losses = []\n",
    "#         # initialize centroids list with random points from initializations\n",
    "#         centroids = np.zeros(data_size)\n",
    "#         for j in initializations: \n",
    "#             centroids[i] = i\n",
    "        \n",
    "#         #kmeans algorithm loop: \n",
    "#         convergence = True\n",
    "#         while convergence: \n",
    "#             for k in range(data_size):\n",
    "#                 euclidean_distances =  np.linalg.norm(data[k] - centroids, axis=1) #compute euclidean distance for each point in data to each centroid\n",
    "#                 closest_cluster = np.argmin(euclidean_distances) # find the min euclidean distance to find which cluster point belongs to\n",
    "#                 closest_clusters[k] = closest_cluster               \n",
    "                \n",
    "#             prev_centroids = np.copy(centroids) # temp copy of the previous array of centroids for comparisons\n",
    "            \n",
    "#             loss_val = 0\n",
    "            \n",
    "#             for l in range(K):\n",
    "#                 cluster_points = []\n",
    "#                 for m in range(data_size):\n",
    "#                     if closest_cluster[m] == l:\n",
    "#                         cluster_points.append(data[m])\n",
    "                        \n",
    "#                 centroids[l] = np.mean(cluster_points, axis=0)\n",
    "#                 for point in cluster_points:\n",
    "#                     loss_val += np.linal.norm(point - centroids[l])\n",
    "                \n",
    "#             loss_val /= data_size\n",
    "#             if np.all(np.equal(prev_centroids, centroids)):\n",
    "#                 convergence = False\n",
    "        \n",
    "#         # update mean square errors with the loss value of the current iteration:        \n",
    "#         mean_square_errors.append(loss_val)\n",
    "#         # updating return lists: \n",
    "#         return_centroids.append(centroids)\n",
    "#         return_clusters.append(closest_clusters)\n",
    "#         losses.append(loss_val)\n",
    "#         final_losses.append(loss_val[-1])\n",
    "        \n",
    "        \n",
    "#         # Plotting per iteration:\n",
    "#         print(\"Plot for iteration: \", (i + 1))\n",
    "#         plt.plot(loss_val)\n",
    "#         plt.show()\n",
    "        \n",
    "#     # Mean square error plots: \n",
    "#     iteration = list(range(0, M - 1))\n",
    "#     plt.plot(iteration, mean_square_errors)\n",
    "#     plt.show()\n",
    "    \n",
    "#     min = final_losses.index(min(final_losses))\n",
    "#     return [final_centroids[min], final_clusters[min], losses[min], final_losses]\n",
    "\n",
    "def my_kmeans(data, K, M):\n",
    "    \"\"\"\n",
    "    Perform a k-means clustering of the 2000 images of digits.\n",
    "\n",
    "    Args:\n",
    "        matrix : data matrix containing information about all of our images\n",
    "        K : number of clusters\n",
    "        M : number of initializations\n",
    "\n",
    "    Returns a list containing the following:\n",
    "        1) The K centroids and cluster assignments for the best solution with the lowest loss function\n",
    "        2) The (non-increasing) sequence of values of the loss-function over k-means iterations for the best solution\n",
    "        3) The set of N terminal loss-function values for all initializations\n",
    "    \"\"\"\n",
    "    num_points = len(data)\n",
    "    \n",
    "    # need to convert dataset to a 1-D array so we can easily iterate through it\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # lists to hold our return values\n",
    "    final_clusters = []\n",
    "    final_centroids = []\n",
    "    losses = []\n",
    "    final_losses = []\n",
    "    \n",
    "    # list to hold our MSEs for the 15 iterations\n",
    "    MSEs = []\n",
    "    \n",
    "    # run outer for loop depending on M initializations\n",
    "    for i in range(M):\n",
    "        # defines which of the K centroids each row of the matrix is closest to\n",
    "        class_list = np.zeros(num_points)\n",
    "        \n",
    "        # initialize our random K centroids\n",
    "        inits = np.random.choice(2000, K, replace=False)\n",
    "        \n",
    "        # fill initial centroid list with K random points from the matrix\n",
    "        centroids = np.asarray([data[index] for index in inits])\n",
    "\n",
    "        loss = []\n",
    "        while True:\n",
    "            # calculate the distance of all instances to the K centroids and assign instances to closest centroid\n",
    "            # in other words, recompute cluster membership\n",
    "            for j in range(num_points):\n",
    "                # caluclate the Euclidean distance for all of our data points\n",
    "                distances = np.linalg.norm(data[j]-centroids, axis=1)\n",
    "\n",
    "                # assign each data point to the closest cluster centroid\n",
    "                cluster = np.argmin(distances)\n",
    "                \n",
    "                # keep track of which cluster each data point is in\n",
    "                class_list[j] = cluster\n",
    "                \n",
    "            # make a copy of our current centroids so we can compare to the new ones later\n",
    "            test = np.copy(centroids)\n",
    "            \n",
    "            #initialize our loss value for this iteration\n",
    "            loss_value = 0\n",
    "            \n",
    "            # calculate our loss function; recompute the K centroids\n",
    "            for j in range(K):\n",
    "                # find all points associated with respective centroid\n",
    "                cluster_points = []\n",
    "                for m in range(num_points):\n",
    "                    if class_list[m] == j:\n",
    "                        cluster_points.append(dataset[m])\n",
    "                \n",
    "                # calculate our new centroids\n",
    "                centroids[j] = np.mean(cluster_points, axis=0)\n",
    "                \n",
    "                # calculate mean square error (MSE) for each point\n",
    "                for item in cluster_points:\n",
    "                    loss_value += np.linalg.norm(item-centroids[j])\n",
    "            loss_value = loss_value / num_points\n",
    "            loss.append(loss_value)\n",
    "            \n",
    "            # if all new centroids are equal to the old centroids, we can stop our while loop; this indicates convergence\n",
    "            if np.all(np.equal(test, centroids)):\n",
    "                break\n",
    "        \n",
    "        # MSE will be final loss value of respective iteration\n",
    "        MSEs.append(loss_value)\n",
    "                \n",
    "        # append and plot our results\n",
    "        final_centroids.append(centroids)\n",
    "        final_clusters.append(cluster_points)\n",
    "        losses.append(loss)\n",
    "        final_losses.append(loss[-1])\n",
    "        \n",
    "        # plot our loss results for the respective iteration\n",
    "        plt.plot(loss)\n",
    "        plt.title(\"Iteration: {}\".format(i+1))\n",
    "        plt.xlabel(\"K-value\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    # plot our MSEs over the M iterations\n",
    "    x = []\n",
    "    for iteration in range(M):\n",
    "        x.append(iteration)\n",
    "    plt.plot(x, MSEs)    \n",
    "    plt.title(\"MSE Values over M Iterations\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.show()\n",
    "    \n",
    "    min_index = final_losses.index(min(final_losses))\n",
    "    best_values = [final_centroids[min_index], final_clusters[min_index], losses[min_index], final_losses]\n",
    "    return best_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6481db6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (784,) (2000,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m kmeans_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m kmeans_results \u001b[38;5;241m=\u001b[39m \u001b[43mmy_kmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_initializations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_clusters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mmy_kmeans\u001b[1;34m(data, K, M)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m convergence: \n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data_size):\n\u001b[1;32m---> 30\u001b[0m         euclidean_distances \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#compute euclidean distance for each point in data to each centroid\u001b[39;00m\n\u001b[0;32m     31\u001b[0m         closest_cluster \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(euclidean_distances) \u001b[38;5;66;03m# find the min euclidean distance to find which cluster point belongs to\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         closest_clusters[k] \u001b[38;5;241m=\u001b[39m closest_cluster               \n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,) (2000,) "
     ]
    }
   ],
   "source": [
    "kmeans_results = []\n",
    "kmeans_results = my_kmeans(X_training, num_of_initializations, num_of_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad80f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
